---
layout: default
title: "Driving Simulation from Real-World Data: How Well Can We Render and Drive?"
description: RealADSim @ ICCV 2025
---

:wave: Welcome to the RealADSim Workshop organized at :wave: 
[<img src="assets/imgs/iccv2025.png" width="400px" alt="iccv2025"/>](https://iccv.thecvf.com/)
{: .text-center}

Join us on **19 Oct 2025** from **09:00 - 12:15 HST** at **305A**
{: .text-center}

**Introduction:** Given the safety concerns and high costs associated with real-world autonomous driving testing, high-fidelity simulation techniques have become crucial for advancing the capabilities of autonomous systems. While classical driving simulators offer closed-loop evaluation, they still exhibit a domain gap compared to the real world. In contrast, offline-collected driving datasets avoid this gap but struggle to provide closed-loop evaluation. Novel View Synthesis (NVS) has recently opened up new possibilities by enabling closed-loop driving simulation directly from real-world data, which has attracted great attention. This creates a promising alternative for evaluating autonomous driving algorithms in dynamic, interactive environments. However, while NVS-based simulation unlocks new opportunities, two key questions are yet to be answered: 1) How well can we render? 2) How well can we drive?

<img src="assets/imgs/teaser_left.jpg" alt="teaser_left.jpg" style="width: 49%; display: inline-block; vertical-align: top;">
<img src="assets/imgs/teaser_right.jpg" alt="teaser_right.jpg" style="width: 49%; display: inline-block; vertical-align: top;">

<!-- 
## :page_facing_up: **Paper**
[![paper](assets/imgs/paper3.png)](https://arxiv.org/abs/2412.01718)
<small>Zhou, H., ...,  Liao, Y. (2024). HUGSIM: A Real-Time, Photo-Realistic and Closed-Loop Simulator for Autonomous Driving.</small>

[![paper](assets/imgs/paper2.png)](https://arxiv.org/abs/2502.15635)
<small>Ni, Z., ..., Yang, S. (2025). Para-Lane: Multi-Lane Dataset Registering Parallel Scans for Benchmarking Novel View Synthesis.</small>

[![paper](assets/imgs/paper1.png)](https://arxiv.org/abs/2412.05256)
<small>Han, X., ..., Li, Y. (2024). Extrapolated Urban View Synthesis Benchmark.</small>
-->


[//]: # (## :tv: **Video**)

[//]: # ()
[//]: # (<!-- #### Coming soon... -->)

## :newspaper: **News** {#news}
- **30 Jun 2025 ---** :rocket: The Workshop website is launched.
- **11 Mar 2025 ---** :gift: The Workshop is accepted!

---

## :hourglass_flowing_sand: **Important Dates** {#dates}

**The challenge submission deadline is 23:59 AoE on September 2025 — judging will be based on submission time. **

- **30 Jun 2025 ---** Challenge Release
- **Extend to 15 Sep 2025 ---** Challenge Submission Due
- **Extend to 20 Sep 2025 ---** Release Results & Submit Technical Report
- **Extend to 05 Oct 2025 ---** Technical Report Due
<!-- 
～～- **31 Aug 2025 ---**～～ - **Extend to 15 Sep 2025 ---** Challenge Submission Due
～～- **05 Sep 2025 ---**～～ - **Extend to 20 Sep 2025 ---** Release Results & Submit Technical Report
～～- **20 Sep 2025 ---**～～ - **Extend to 05 Oct 2025 ---** Technical Report Due
-->

To be eligible for awards, teams are required to submit a technical report of no more than 4 pages. Please note that these reports will not be included in the official ICCV proceedings.

---

## :calendar: **Schedule** {#schedule}

The workshop will take place on **19 Oct 2025** from **09:00 - 12:15 HST**.

> **NOTE**: Times are shown in **Hawaii Standard Time**. 
Please take this into account if you plan to join the workshop virtually.

| Time (PDT)    | Event                                                        |
|---------------|--------------------------------------------------------------|
| 09:00 - 09:10 | Welcome & Introduction                                       |
| 09:10 - 09:40 | Keynote-1 [**Yue Wang**](https://yuewang.xyz/)               |
| 09:40 - 10:10 | Keynote-2 [**Peter Kontschieder**](https://www.linkedin.com/in/peter-kontschieder-2a6410134/)                             |
| 10:10 - 11:00 | Awards / Challenge winner Presentation                       |
| 11:00 - 11:10 | Tea Break                                                    |
| 11:10 - 11:40 | Keynote-3 [**​Yuexin Ma**](https://yuexinma.me/)              |
| 11:40 - 12:10 | Keynote-4 [**Runsheng Xu**](https://derrickxunu.github.io/)  |
| 12:10 - 12:15 | Closing remarks                                              |

---

## :microphone: **Invited Speakers** {#speakers}
<div class="speakers-container">
<figure>
    <a href="https://yuewang.xyz/">
    <img class="img-author" src="https://yuewang.xyz/assets/img/avatar.png" alt="Yue Wang"/></a>
    <b><br><a href="https://yuewang.xyz/">Yue Wang</a>
    <br>Assistant Professor<br>University of Southern California</b>
</figure>

<figure>
    <a href="https://www.linkedin.com/in/peter-kontschieder-2a6410134/">
    <img class="img-author" src="assets/imgs/authors/peter_kontschieder.jpeg" alt="Peter Kontschieder"/></a>
    <b><br><a href="https://www.linkedin.com/in/peter-kontschieder-2a6410134/">Peter Kontschieder</a>
    <br>Research Director<br>Meta</b>
</figure>

<figure>
    <a href="https://yuexinma.me/">
    <img class="img-author" src="https://yuexinma.me/zxg_css/self.jpg" alt="​Yuexin Ma"/></a>
    <b><br><a href="https://yuexinma.me/">​Yuexin Ma</a>
    <br>Assistant Professor<br>ShanghaiTech University</b>
</figure>

<figure>
    <a href="https://derrickxunu.github.io/">
    <img class="img-author" src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=QW6Ro8IAAAAJ&citpid=1" alt="Runsheng Xu"/></a>
    <b><br><a href="https://derrickxunu.github.io/">Runsheng Xu</a>
    <br>Research Scientist<br>Waymo</b>
</figure>

</div>

[**Yue Wang**](https://yuewang.xyz/)
 is an Assistant Professor at USC CS, leading the Geometry, Vision, and Learning Lab. His current focus includes simulation, perception, and decision making. He obtained the Ph.D. degree from MIT EECS in 2022.

[**Peter Kontschieder**](https://www.linkedin.com/in/peter-kontschieder-2a6410134/)
 is the Director of Research at Meta. He received his PhD in 2013 from Graz University of Technology. His research interests include photorealistic 3D scene reconstruction, semantic scene understanding, image-based 3D modeling, and generative models for 3D synthesis.

[**​Yuexin Ma**](https://yuexinma.me/)
 is an Assistant Professor in SIST, Shang- haiTech University. She received the PhD degree from the University of Hong Kong in 2019. Her current research focuses on scene understanding, multi-modal learning, autonomous driving, and embodied AI.

[**Runsheng Xu**](https://derrickxunu.github.io/)
 is a Senior Research Scientist at Waymo, working on Multi-modal Large Laungage Models for autonomous driving. His research interests are in the intersection of autonomous driving, large language models, diffusion models, and multi-agent intelligence.

---

## :checkered_flag: **Competitions** {#competitions}

<img src="assets/imgs/teaser_left.jpg" alt="teaser_left.jpg" style="width: 49%; display: inline-block; vertical-align: top;">
<img src="assets/imgs/teaser_right.jpg" alt="teaser_right.jpg" style="width: 49%; display: inline-block; vertical-align: top;">

**Tracks**

We are holding two tracks in the workshop competitions:
- [Track 1: Extrapolated Urban Novel View Synthesis](https://huggingface.co/spaces/XDimLab/ICCV2025-RealADSim-NVS)

    In this track, we investigate the question: how well can we render? While NVS methods have made significant progress in generating photorealistic urban scenes, their performance still lags in extrapolated viewpoints when only a limited viewpoint is provided during training. However, extrapolated viewpoints are essential for closed-loop simulation. Improving the accuracy and consistency of NVS across diverse viewing angles is critical for ensuring that these simulators provide reliable environments for driving evaluation.
- [Track 2: Autonomous Driving in a Photorealistic Simulator](https://huggingface.co/spaces/XDimLab/ICCV2025-RealADSim-ClosedLoop)

    In this track, we investigate the question: how well can we drive? Despite challenges in extrapolated viewpoint rendering, existing methods enable photorealistic simulators with reasonable performance when trained on dense views. These NVS-based simulators allow autonomous driving models to be tested in a fully closed-loop manner, bridging the gap between real-world data and interactive evaluation. This shift allows for benchmarking autonomous driving algorithms under realistic conditions, overcoming the limitations of static datasets.

**How to Participate**

To participate in the competition, both automatic registration and manual verification are required:
1. Click the "Login with Huggingface" button.
2. Click the "Register" button and complete the registration form. After this automatic registration step, the "Submission Information" page will become accessible. It provides detailed instructions on how to run local tests and submit your proposal.
3. Access to "My Submissions" and "New Submission" will be granted after we manually review your registration and authorize your account. This process is typically completed within 24 hours.

---

## 💰 **Awards** {#award}

Each competition will have the following awards:

- Innovation Award: $9,000
- Outstanding Champion: $9,000
- Honorable Runner-up: $3,000

Winners will be announced at the Workshop @ ICCV 2025.

## :trophy: **Challenge Winners** {#winners}

### Track 1: Extrapolated Urban Novel View Synthesis

<div class="table-container">
<table class="track-table track1-table" id="track1-table">
<thead>
<tr>
<th>Rank</th>
<th>Team Name</th>
<th>PSNR</th>
<th>SSIM</th>
<th>LPIPS</th>
<th>Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>🥇 1</td>
<td>XiaomiEV Team</td>
<td>18.228</td>
<td>0.514</td>
<td>0.288</td>
<td>0.441</td>
</tr>
<tr>
<td>🥈 2</td>
<td>Qualcomm AI Research</td>
<td>17.887</td>
<td>0.492</td>
<td>0.289</td>
<td>0.432</td>
</tr>
<tr>
<td>🥉 3</td>
<td>Qvyon</td>
<td>18.009</td>
<td>0.496</td>
<td>0.361</td>
<td>0.413</td>
</tr>
<tr>
<td>💡 4</td>
<td>XiaomiEV Team</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>

<div class="table-pagination">
<div class="table-pagination-controls">
<button class="pagination-btn" onclick="changePage('track1', -1)" id="track1-prev">←</button>
<button class="pagination-btn" onclick="changePage('track1', 1)" id="track1-next">→</button>
</div>
<div class="pagination-info" id="track1-info">1/1</div>
</div>
</div>

🥇 The outstanding Champion goes to team **XiaomiEV Team**. More details are in the [technical report](#) and [presentation](#).

💡 Team **XiaomiEV Team** receives the Innovation Award. More details are in the [technical report](#) and [presentation](#).


### Track 2: Autonomous Driving in a Photorealistic Simulator

<div class="table-container">
<table class="track-table track2-table" id="track2-table">
<thead>
<tr>
<th>Rank</th>
<th>Team Name</th>
<th>RC</th>
<th>HD-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>🥇 1</td>
<td>UT/NV</td>
<td>0.5905</td>
<td>0.419</td>
</tr>
<tr>
<td>🥈 2</td>
<td>NVIDIA/FDU</td>
<td>0.4601</td>
<td>0.4012</td>
</tr>
<tr>
<td>🥉 3</td>
<td>BranchOut</td>
<td>0.395</td>
<td>0.3016</td>
</tr>
<tr>
<td>💡 4</td>
<td>NVIDIA/FDU</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>

<div class="table-pagination">
<div class="table-pagination-controls">
<button class="pagination-btn" onclick="changePage('track2', -1)" id="track2-prev">←</button>
<button class="pagination-btn" onclick="changePage('track2', 1)" id="track2-next">→</button>
</div>
<div class="pagination-info" id="track2-info">1/1</div>
</div>
</div>

🥇 The outstanding Champion goes to team **UT/NV**. More details are in the [technical report](#) and [presentation](#).

💡 Team **NVIDIA/FDU** receives the Innovation Award. More details are in the [technical report](#) and [presentation](#).


## 🤵 **Organizers** {#organizers}
<div class="container">
<figure>
    <a href="https://yiyiliao.github.io/">
    <img class="img-author" src="https://yiyiliao.github.io/assets/img/prof_pic.jpg" alt="Yiyi Liao"/></a>
    <b><br><a href="https://yiyiliao.github.io/">Yiyi Liao</a>
    <br>Zhejiang University</b>
</figure>
    
<figure>
    <a href="https://hyzhou404.github.io/">
    <img class="img-author" src="assets/imgs/authors/hongyu_zhou.jpg" alt="Hongyu Zhou"/></a>
    <b><br><a href="https://hyzhou404.github.io/">Hongyu Zhou</a>
    <br>Zhejiang University</b>
</figure>

<figure>
    <a href="https://yichonglu.github.io/">
    <img class="img-author" src="assets/imgs/authors/Yichong_Lu.jpg" alt="Yichong Lu"/></a>
    <b><br><a href="https://yichonglu.github.io/">Yichong Lu</a>
    <br>Zhejiang University</b>
</figure>

<figure>
    <a href="https://www.aminer.cn/profile/liu-bingbing/562d234145cedb3398d63523">
    <img class="img-author" src="assets/imgs/authors/Bingbing_Liu.png" alt="Bingbing Liu"/></a>
    <b><br><a href="https://www.aminer.cn/profile/liu-bingbing/562d234145cedb3398d63523">Bingbing Liu</a>
    <br>Huawei</b>
</figure>

<figure>
    <a href="https://ieeexplore.ieee.org/author/37859161500">
    <img class="img-author" src="assets/imgs/authors/hongbo_zhang.jpg" alt="Hongbo Zhang"/></a>
    <b><br><a href="https://ieeexplore.ieee.org/author/37859161500">Hongbo Zhang</a>
    <br>Huawei</b>
</figure>

<figure>
    <a href="https://openreview.net/profile?id=~Jiansheng_Wei1">
    <img class="img-author" src="assets/imgs/authors/jiansheng_wei.png" alt="Jiansheng Wei"/></a>
    <b><br><a href="https://openreview.net/profile?id=~Jiansheng_Wei1">Jiansheng Wei</a>
    <br>Huawei</b>
</figure>



<figure>
    <a href="https://openreview.net/profile?id=%7EZiqian_Ni1">
    <img class="img-author" src="assets/imgs/authors/ziqian_ni.jpg" alt="Ziqian Ni"/></a>
    <b><br><a href="https://openreview.net/profile?id=%7EZiqian_Ni1">Ziqian Ni</a>
    <br>Cainiao</b>
</figure>

<figure>
    <a href="https://yimingli-page.github.io/">
    <img class="img-author" src="assets/imgs/authors/yiming_li.jpeg" alt="Yiming Li"/></a>
    <b><br><a href="https://yimingli-page.github.io/">Yiming Li</a>
    <br>NVIDIA & NYU</b>
</figure>

<figure>
    <a href="https://www.cvlibs.net/">
    <img class="img-author" src="assets/imgs/authors/Andreas.jpg" alt="Andreas Geiger"/></a>
    <b><br><a href="https://www.cvlibs.net/">Andreas Geiger</a>
    <br>University of Tübingen</b>
</figure>
</div>

<script>
// Pagination functionality
let track1Page = 1;
let track2Page = 1;
const teamsPerPage = 4;

// Sample data for demonstration (you can replace with actual data)
const track1Data = [
  { rank: '🥇 1', team: 'XiaomiEV Team', psnr: '18.228', ssim: '0.514', lpips: '0.288', score: '0.441' },
  { rank: '🥈 2', team: 'Qualcomm AI Research', psnr: '17.887', ssim: '0.492', lpips: '0.289', score: '0.432' },
  { rank: '🥉 3', team: 'Qvyon', psnr: '18.009', ssim: '0.496', lpips: '0.361', score: '0.413' },
  { rank: '💡', team: 'XiaomiEV Team', psnr: '-', ssim: '-', lpips: '-', score: '-' },
  { rank: '4', team: 'MeowAndDoggy', psnr: '17.857', ssim: '0.49', lpips: '0.371', score: '0.407' },
  { rank: '5', team: 'aowei', psnr: '16.72', ssim: '0.484', lpips: '0.401', score: '0.392' },
  { rank: '6', team: 'UneasyDrive', psnr: '17.037', ssim: '0.501', lpips: '0.433', score: '0.389' },
  { rank: '7', team: 'GoGoGo', psnr: '17.626', ssim: '0.523', lpips: '0.468', score: '0.387' },
  { rank: '8', team: 'aaamagicman', psnr: '17.207', ssim: '0.452', lpips: '0.399', score: '0.385' },
  { rank: '9', team: 'shufudui', psnr: '16.492', ssim: '0.474', lpips: '0.414', score: '0.384' },
  { rank: '10', team: 'pizzac', psnr: '15.837', ssim: '0.42', lpips: '0.36', score: '0.382' },
  { rank: '11', team: 'PKUMMCAL', psnr: '16.519', ssim: '0.486', lpips: '0.442', score: '0.379' },
  { rank: '12', team: 'UMCV', psnr: '16.23', ssim: '0.465', lpips: '0.423', score: '0.378' },
  { rank: '13', team: 'UQMM', psnr: '16.313', ssim: '0.451', lpips: '0.416', score: '0.376' },
  { rank: '14', team: 'LoL', psnr: '15.215', ssim: '0.439', lpips: '0.396', score: '0.374' },
  { rank: '15', team: 'PPAP', psnr: '16.161', ssim: '0.474', lpips: '0.464', score: '0.368' },
  { rank: '16', team: 'VIS Team', psnr: '16.016', ssim: '0.463', lpips: '0.46', score: '0.365' },
  { rank: '17', team: 'splatman', psnr: '16.337', ssim: '0.461', lpips: '0.475', score: '0.361' },
  { rank: '18', team: 'brightezt', psnr: '16.015', ssim: '0.449', lpips: '0.495', score: '0.35' },
  { rank: '19', team: 'Anonymous', psnr: '14.388', ssim: '0.402', lpips: '0.438', score: '0.347' }
];

const track2Data = [
  { rank: '🥇 1', team: 'UT/NV', rc: '0.5905', hdscore: '0.419' },
  { rank: '🥈 2', team: 'NVIDIA/FDU', rc: '0.4601', hdscore: '0.4012' },
  { rank: '🥉 3', team: 'BranchOut (BostonUniversity)', rc: '0.395', hdscore: '0.3016' },
  { rank: '💡', team: 'NVIDIA/FDU', rc: '-', hdscore: '-' },
  { rank: '4', team: 'Return0_o', rc: '0.2822', hdscore: '0.2303' },
  { rank: '5', team: 'ding', rc: '0.3539', hdscore: '0.2265' },
  { rank: '7', team: 'Poffusers', rc: '0.3362', hdscore: '0.2128' },
  { rank: '8', team: 'AD-DA', rc: '0.3338', hdscore: '0.21' },
  { rank: '9', team: 'WR Team', rc: '0.3333', hdscore: '0.208' },
  { rank: '10', team: 'Drive-Sim', rc: '0.1178', hdscore: '0.0566' }
];

function updateTable(track, page) {
  const table = document.getElementById(track + '-table');
  const tbody = table.querySelector('tbody');
  const data = track === 'track1' ? track1Data : track2Data;
  const totalPages = Math.ceil(data.length / teamsPerPage);
  
  // Clear existing rows
  tbody.innerHTML = '';
  
  // Calculate start and end indices
  const startIndex = (page - 1) * teamsPerPage;
  const endIndex = Math.min(startIndex + teamsPerPage, data.length);
  
  // Add rows for current page
  for (let i = startIndex; i < endIndex; i++) {
    const row = tbody.insertRow();
    const team = data[i];
    
    if (track === 'track1') {
      row.innerHTML = `
        <td>${team.rank}</td>
        <td>${team.team}</td>
        <td>${team.psnr}</td>
        <td>${team.ssim}</td>
        <td>${team.lpips}</td>
        <td>${team.score}</td>
      `;
    } else {
      row.innerHTML = `
        <td>${team.rank}</td>
        <td>${team.team}</td>
        <td>${team.rc}</td>
        <td>${team.hdscore}</td>
      `;
    }
  }
  
  // Update pagination info
  document.getElementById(track + '-info').textContent = `${page}/${totalPages}`;
  
  // Update button states
  document.getElementById(track + '-prev').disabled = page === 1;
  document.getElementById(track + '-next').disabled = page === totalPages;
}

function changePage(track, direction) {
  if (track === 'track1') {
    track1Page += direction;
    updateTable('track1', track1Page);
  } else {
    track2Page += direction;
    updateTable('track2', track2Page);
  }
}

// Initialize tables on page load
document.addEventListener('DOMContentLoaded', function() {
  updateTable('track1', 1);
  updateTable('track2', 1);
});
</script>