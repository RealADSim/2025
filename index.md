---
layout: default
title: "Driving Simulation from Real-World Data: How Well Can We Render and Drive?"
description: RealADSim @ ICCV 2025
---

:wave: Welcome to the RealADSim Workshop organized at :wave: 
[<img src="assets/imgs/iccv2025.png" width="400px" alt="iccv2025"/>](https://iccv.thecvf.com/)
{: .text-center}

Join us on **19 Oct 2025** from **12:00 - 17:45 HST**
{: .text-center}

**Introduction:** Given the safety concerns and high costs associated with real-world autonomous driving testing, high-fidelity simulation techniques have become crucial for advancing the capabilities of autonomous systems. While classical driving simulators offer closed-loop evaluation, they still exhibit a domain gap compared to the real world. In contrast, offline-collected driving datasets avoid this gap but struggle to provide closed-loop evaluation. Novel View Synthesis (NVS) has recently opened up new possibilities by enabling closed-loop driving simulation directly from real-world data, which has attracted great attention. This creates a promising alternative for evaluating autonomous driving algorithms in dynamic, interactive environments. However, while NVS-based simulation unlocks new opportunities, there are two key questions that are yet to be answered: 1) How well can we render? 2) How well can we drive?

<img src="assets/imgs/teaser_left.jpg" alt="teaser_left.jpg" style="width: 49%; display: inline-block; vertical-align: top;">
<img src="assets/imgs/teaser_right.jpg" alt="teaser_right.jpg" style="width: 49%; display: inline-block; vertical-align: top;">

<!-- 
## :page_facing_up: **Paper**
[![paper](assets/imgs/paper3.png)](https://arxiv.org/abs/2412.01718)
<small>Zhou, H., ...,  Liao, Y. (2024). HUGSIM: A Real-Time, Photo-Realistic and Closed-Loop Simulator for Autonomous Driving.</small>

[![paper](assets/imgs/paper2.png)](https://arxiv.org/abs/2502.15635)
<small>Ni, Z., ..., Yang, S. (2025). Para-Lane: Multi-Lane Dataset Registering Parallel Scans for Benchmarking Novel View Synthesis.</small>

[![paper](assets/imgs/paper1.png)](https://arxiv.org/abs/2412.05256)
<small>Han, X., ..., Li, Y. (2024). Extrapolated Urban View Synthesis Benchmark.</small>
-->


[//]: # (## :tv: **Video**)

[//]: # ()
[//]: # (<!-- #### Coming soon... -->)

## :newspaper: **News** {#news}
- **30 Jun 2025 ---** :rocket: The Workshop website is launched.
- **11 Mar 2025 ---** :gift: The Workshop is accepted!

---

## :hourglass_flowing_sand: **Important Dates** {#dates}

- **30 Jun 2025 (00:00 UTC) ---** Challenge Release
- **31 Aug 2025 (00:00 UTC) ---** Challenge Submission Due
- **05 Sep 2025 (00:00 UTC) ---** Release Results & Submit Technical Report
- **20 Sep 2025 (00:00 UTC) ---** Technical Report Due

---

## :calendar: **Schedule** {#schedule}

The workshop will take place on **19 Oct 2025** from **09:00 - 12:15 HST**.

> **NOTE**: Times are shown in **Hawaii Standard Time**. 
Please take this into account if you plan to join the workshop virtually.

| Time (PDT)    | Event                                                        |
|---------------|--------------------------------------------------------------|
| 09:00 - 09:10 | Welcome & Introduction                                       |
| 09:10 - 09:40 | Keynote-1                                                    |
| 09:40 - 10:10 | Keynote-2                                                    |
| 10:10 - 11:10 | Awards / Challenge winner Presentation                       |
| 11:10 - 11:40 | Keynote-3:                                                   |
| 11:40 - 12:10 | Keynote-4:                                                   |
| 12:10 - 12:15 | Closing remarks                                              |

---

## :microphone: **Invited Speakers** {#speakers}
<div class="speakers-container">
<figure>
    <a href="https://yuewang.xyz/">
    <img class="img-author" src="https://yuewang.xyz/assets/img/avatar.png" alt="Yue Wang"/></a>
    <b><br><a href="https://yuewang.xyz/">Yue Wang</a>
    <br>Assistant Professor<br>University of Southern California</b>
</figure>

<figure>
    <a href="https://yuexinma.me/">
    <img class="img-author" src="https://yuexinma.me/zxg_css/self.jpg" alt="‚ÄãYuexin Ma"/></a>
    <b><br><a href="https://yuexinma.me/">‚ÄãYuexin Ma</a>
    <br>Assistant Professor<br>ShanghaiTech University</b>
</figure>

<figure>
    <a href="https://jyhjinghwang.github.io/">
    <img class="img-author" src="https://jyhjinghwang.github.io/imgs/profile.jpeg" alt="Jyh-Jing Hwang"/></a>
    <b><br><a href="https://jyhjinghwang.github.io/">Jyh-Jing Hwang</a>
    <br>Research Scientist<br>Waymo</b>
</figure>

<figure>
    <a href="https://www.linkedin.com/in/peter-kontschieder-2a6410134/">
    <img class="img-author" src="assets/imgs/authors/peter_kontschieder.jpeg" alt="Peter Kontschieder"/></a>
    <b><br><a href="https://www.linkedin.com/in/peter-kontschieder-2a6410134/">Peter Kontschieder</a>
    <br>Research Director<br>Meta</b>
</figure>
</div>

[**Yue Wang**](https://yuewang.xyz/)
 is an Assistant Professor at USC CS, leading the Geometry, Vision, and Learning Lab. His current focus includes simulation, perception, and decision making. He obtained the Ph.D. degree from MIT EECS in 2022.

[**‚ÄãYuexin Ma**](https://yuexinma.me/)
 is an Assistant Professor in SIST, Shang- haiTech University. She received the PhD degree from the University of Hong Kong in 2019. Her current research focuses on scene understanding, multi-modal learning, autonomous driving, and embodied AI.

[**Jyh-Jing Hwang**](https://jyhjinghwang.github.io/)
 is a Research Scientist at Waymo Research, a technical lead for end-to-end autonomous driving. He received his Ph.D. degree in Computer and Information Science from the University of Pennsylvania.

[**Peter Kontschieder**](https://www.linkedin.com/in/peter-kontschieder-2a6410134/)
 is the Director of Research at Meta. He received his PhD in 2013 from Graz University of Technology. His research interests include photorealistic 3D scene reconstruction, semantic scene understanding, image-based 3D modeling, and generative models for 3D synthesis.

---

## :checkered_flag: **Competitions** {#competitions}

<img src="assets/imgs/teaser_left.jpg" alt="teaser_left.jpg" style="width: 49%; display: inline-block; vertical-align: top;">
<img src="assets/imgs/teaser_right.jpg" alt="teaser_right.jpg" style="width: 49%; display: inline-block; vertical-align: top;">

**Tracks**

We are holding two tracks in the workshop competitions:
- [Track 1: Extrapolated Urban Novel View Synthesis](https://huggingface.co/spaces/XDimLab/ICCV2025-RealADSim-NVS)

    In this track, we want to answer the question: how well can we Render? While NVS methods have made significant progress in generating photorealistic urban scenes, their performance still lags in extrapolated viewpoints when only a limited viewpoint is provided during training. However, extrapolated viewpoints are essential for closed-loop simulation. Improving the accuracy and consistency of NVS across diverse viewing angles is critical for ensuring that these simulators provide reliable environments for driving evaluation.
- [Track 2: Autonomous Driving in a Photorealistic Simulator](https://huggingface.co/spaces/XDimLab/ICCV2025-RealADSim-ClosedLoop)

    In this track, we want to answer the question: how well can we Drive? Despite challenges in extrapolated viewpoint rendering, existing methods enable photorealistic simulators with reasonable performance when trained on dense views. These NVS-based simulators allow autonomous driving models to be tested in a fully closed-loop manner, bridging the gap between real-world data and interactive evaluation. This shift allows for benchmarking autonomous driving algorithms under realistic conditions, overcoming the limitations of static datasets.

---

## üí∞ **Award** {#award}

Each competition will have the following awards:

- Innovation Award: $9,000
- Outstanding Champion: $9,000
- Honorable Runner-up: $3,000

Winners will announce at ICCV2025 Workshop.

## üèÜ **Competition Winners** {#winners}
<!-- 
Congratulations to the challenge winners -- **HRI**!

|                |      | F&nbsp;(‚Üë)                                                                | F&nbsp;(‚Üë)<br/>(Edges)                                                    | MAE&nbsp;(‚Üì)                                                             | RMSE&nbsp;(‚Üì)                                                            | AbsRel&nbsp;(‚Üì)                                                              | Acc&nbsp;(‚Üë)<br/>(Edges)                                                 | Comp&nbsp;(‚Üì)<br/>(Edges)                                                | Œ¥<1.25&nbsp;(‚Üë)                                                | Œ¥<1.25^2&nbsp;(‚Üë)                                                | Œ¥<1.25^3&nbsp;(‚Üë)                                                | 
|----------------|------|------------------------------------------------------------------|------------------------------------------------------------------|-----------------------------------------------------------------|-----------------------------------------------------------------|------------------------------------------------------------------|-----------------------------------------------------------------|-----------------------------------------------------------------|-----------------------------------------------------------------|-----------------------------------------------------------------|-----------------------------------------------------------------|
| **HRI** | A |   <span style="background-color: #BDE6CD; color: black;"><strong>23.05</strong></span>  |  <span style="background-color: #E2EEBC; color: black;">9.37</span>  |  <span style="background-color: #BDE6CD; color: black;"><strong>3.19</strong></span>  |  <span style="background-color: #BDE6CD; color: black;"><strong>5.64</strong></span>  |  <span style="background-color: #E2EEBC; color: black;">21.17</span>  | 3.30 |  <span style="background-color: #E2EEBC; color: black;">6.77</span> |  <span style="background-color: #E2EEBC; color: black;">77.54</span>  |  <span style="background-color: #E2EEBC; color: black;">91.79</span>  |  <span style="background-color: #FFF8C5; color: black;">95.78</span>   |
| **Anonymous** | | <span style="background-color: #E2EEBC; color: black;">22.95</span>  | 9.21  | 3.44  | 6.48  | 24.65 | 3.27 |  <span style="background-color: #BDE6CD; color: black;"><strong>6.59</strong></span>  |  <span style="background-color: #FFF8C5; color: black;">76.17</span>  | 90.33  | 94.73    | 
| <span style="background-color: rgb(255, 200, 200); color: black;"><strong>PICO-MR</strong></span> | <span style="background-color: rgb(255, 200, 200); color: black;"> </span> | <span style="background-color: rgb(255, 200, 200); color: black;">22.58</span>  | <span style="background-color: rgb(255, 200, 200); color: black;">8.26</span>  | <span style="background-color: rgb(255, 200, 200); color: black;">3.41</span>  | <span style="background-color: rgb(255, 200, 200); color: black;">5.89</span>  | <span style="background-color: rgb(255, 200, 200); color: black;">22.02</span>  | <span style="background-color: rgb(255, 200, 200); color: black;">4.02</span>  | <span style="background-color: rgb(255, 200, 200); color: black;">11.28</span>  | <span style="background-color: rgb(255, 200, 200); color: black;">76.18</span>  | <span style="background-color: rgb(255, 200, 200); color: black;">90.95</span>  | <span style="background-color: rgb(255, 200, 200); color: black;">95.44</span> |
| **Lavreniuk** | A | <span style="background-color: #FFF8C5; color: black;">20.81</span>  | 9.12  |  <span style="background-color: #FFF8C5; color: black;">3.40</span>  |  <span style="background-color: #FFF8C5; color: black;">5.77</span>  |  <span style="background-color: #BDE6CD; color: black;"><strong>20.48</strong></span>  | 3.20  |  <span style="background-color: #FFF8C5; color: black;">7.27</span>  | 75.33  |  <span style="background-color: #FFF8C5; color: black;">91.33</span>  |  <span style="background-color: #BDE6CD; color: black;"><strong>95.97</strong></span>    |
| **Mach-Calib** | A |20.69  | 8.75  | 3.63  | 6.78  | 23.36  | 3.22  | 13.96  | 74.31  | 90.44  | 95.12     |
| **Anonymous** | | 20.50  | 8.63  | 3.68  | 7.07  | 23.96  | 3.17  | 13.66  | 74.00  | 90.32  | 95.01    |
| **EasyMono** | A | 20.24  | 7.30  | 3.43  | 5.91  | 22.47  | 3.91  | 20.98  | 74.73  | 90.55  | 95.24    |
| **Insta360-Percep** | A | 19.41  | 8.98  |  <span style="background-color: #E2EEBC; color: black;">3.25</span> |  <span style="background-color: #E2EEBC; color: black;">5.73</span>  |  <span style="background-color: #FFF8C5; color: black;">21.29</span>  |  <span style="background-color: #BDE6CD; color: black;"><strong>2.75</strong></span>  | 15.54  |  <span style="background-color: #BDE6CD; color: black;">77.91</span>  |  <span style="background-color: #BDE6CD; color: black;">92.03</span>  |  <span style="background-color: #E2EEBC; color: black;">95.91</span>     |
| **Anonymous** | | 18.85  | 8.27  | 4.06  | 7.06  | 26.35  | 3.11  | 13.75  | 67.57  | 87.98  | 94.18     |
| **HIT-AIIA** | A |18.53  |  <span style="background-color: #BDE6CD; color: black;">9.74</span>  | 3.83  | 6.29  | 23.96  |  <span style="background-color: #FFF8C5; color: black;">3.06</span> | 9.90  | 69.12  | 88.49  | 94.31     |
| **Anonymous** | | 18.16  | 6.32  | 3.57  | 6.13  | 24.07  | 4.51  | 15.75  | 71.62  | 89.12  | 94.74     |
| **Anonymous** | | 17.46  |  <span style="background-color: #FFF8C5; color: black;">9.35</span> | 4.43  | 7.30  | 29.19  | 3.27  | 17.69 | 65.64  | 86.45  | 93.09     |
| **Anonymous** | | 17.46  |  <span style="background-color: #FFF8C5; color: black;">9.35</span> | 4.43  | 7.30  | 29.19  | 3.27  | 17.69  | 65.64  | 86.45  | 93.09    |
| **Anonymous** | | 17.45  |  <span style="background-color: #FFF8C5; color: black;">9.35</span>  | 4.43  | 7.30  | 29.20  | 3.27  | 17.69  | 65.64  | 86.46  | 93.10     |
| **Robot02-vRobotit** | A |17.25  | 8.27  | 3.90  | 6.54  | 23.64  | 3.07  | 22.85  | 71.07  | 89.38  | 94.80     |
| **Anonymous** | | 17.01  | 9.20  | 4.44  | 7.34  | 29.42  | 3.25  | 18.55  | 65.72  | 86.67  | 93.35     |
| **Anonymous** | | 17.01  | 9.20  | 4.44  | 7.34  | 29.42  | 3.25 | 18.55  | 65.72  | 86.67  | 93.35     |
| <span style="background-color: rgb(235, 235, 255); color: black;">**Marigold**</span> | <span style="background-color: rgb(235, 235, 255); color: black;">A</span> | <span style="background-color: rgb(235, 235, 255); color: black;">17.01</span>  | <span style="background-color: rgb(235, 235, 255); color: black;">9.19</span>  | <span style="background-color: rgb(235, 235, 255); color: black;">4.44</span>  | <span style="background-color: rgb(235, 235, 255); color: black;">7.34</span>  | <span style="background-color: rgb(235, 235, 255); color: black;">29.42</span>  | <span style="background-color: rgb(235, 235, 255); color: black;">3.25</span>  | <span style="background-color: rgb(235, 235, 255); color: black;">18.56</span>  | <span style="background-color: rgb(235, 235, 255); color: black;">65.72</span>  | <span style="background-color: rgb(235, 235, 255); color: black;">86.67</span>  | <span style="background-color: rgb(235, 235, 255); color: black;">93.35</span> |
| **Anonymous** | | 16.79  | 9.10  | 4.51  | 7.44  | 30.10  | 3.32  | 17.65  | 65.24  | 86.22  | 93.05     |
| **Anonymous** | | 16.67  | 7.49  | 4.01  | 6.92  | 26.34  | 3.07  | 26.32  | 68.17  | 88.47  | 94.42     |
| **Anonymous** | | 16.47  | 8.99  | 4.74  | 7.71  | 30.93  | 3.26  | 18.75  | 62.26  | 84.27  | 91.90    |
| **ViGIR LAB** | | 16.25  | 8.54  | 5.37  | 9.00 | 43.72  | 3.29  | 14.15  | 61.99  | 82.89  | 90.46     |
| **Anonymous** | A |15.31  | 8.46  | 4.62  | 7.46  | 30.63  | 3.42  | 15.59  | 62.41  | 84.89  | 92.43     |
| **Anonymous** | | 15.03  | 7.64  | 5.48  | 9.29  | 38.59  | 3.70  | 20.33  | 58.82  | 81.79  | 90.85    |
| <span style="background-color: rgb(235, 235, 255); color: black;">**Depth Anything v2**</span>  | <span style="background-color: rgb(235, 235, 255); color: black;"> </span> | <span style="background-color: rgb(235, 235, 255); color: black;">14.34</span>  | <span style="background-color: rgb(235, 235, 255); color: black;">6.72</span>  | <span style="background-color: rgb(235, 235, 255); color: black;">4.84</span>  | <span style="background-color: rgb(235, 235, 255); color: black;">9.13</span>  | <span style="background-color: rgb(235, 235, 255); color: black;">33.57</span>  | <span style="background-color: #E2EEBC; color: black;">2.99</span>  | <span style="background-color: rgb(235, 235, 255); color: black;">35.54</span>  | <span style="background-color: rgb(235, 235, 255); color: black;">66.34</span>  | <span style="background-color: rgb(235, 235, 255); color: black;">86.02</span>  | <span style="background-color: rgb(235, 235, 255); color: black;">92.44</span> |
| **HCMUS-DepthFusion** | A | 14.20  | 7.81  | 4.90  | 7.96  | 33.55  | 3.32  | 17.66  | 61.09  | 83.90  | 92.21     |
| **ReadingLS** | | 13.52  | 6.60  | 4.56  | 7.90 | 30.15  | 3.14  | 35.94  | 65.34  | 86.55  | 93.46    |

_Legend: <span style="background-color: rgb(235, 235, 255); color: black;">**Baselines**</span>; <span style="background-color: rgb(255, 200, 200); color: black;">**3rd MDEC winning team**</span>; A -- affine-invariant predictions_; for each metric we highlight <span style="background-color: #BDE6CD; color: black;">**absolute best**</span>, <span style="background-color: #E2EEBC; color: black;">second best</span> and <span style="background-color: #FFF8C5; color: black;">third best</span> -->

---

[//]: # (## :checkered_flag: **Challenge** {#challenge})

<!-- The challenge focuses on evaluating novel MDE techniques on the [SYNS-Patches dataset](https://arxiv.org/abs/2208.01489).
This dataset provides a challenging variety of urban and natural scenes, including forests, agricultural settings, residential streets, industrial estates, lecture theatres, offices, and more.
Furthermore, the high-quality, dense ground-truth LiDAR allows for the computation of more informative evaluation metrics, such as those focused on [depth discontinuities](https://arxiv.org/abs/1805.01328v1).

**[[GitHub Starter Pack](https://github.com/toshas/mdec_benchmark)] --- [[CodaLab Challenge](https://codalab.lisn.upsaclay.fr/competitions/21305)]**
{: .text-center}

<div class="container">
<img class="img-syns" src="assets/imgs/syns/image_0551.png" alt="image_0551"/>
<img class="img-syns" src="assets/imgs/syns/image_0893.png" alt="image_0893"/>
<img class="img-syns" src="assets/imgs/syns/image_1114.png" alt="image_1114"/>

<img class="img-syns" src="assets/imgs/syns/depth_0551.png" alt="depth_0551"/>
<img class="img-syns" src="assets/imgs/syns/depth_0893.png" alt="depth_0893"/>
<img class="img-syns" src="assets/imgs/syns/depth_1114.png" alt="depth_1114"/>
</div>

### ‚ö° What‚Äôs new in MDEC 2025? {#news}

- üìê New prediction types: The challenge became more accessible thanks to the added support of `affine-invariant` predictions. `metric` and `scale-invariant` predictions are also automatically supported. `disparity` predictions, which were supported in previous challenges, are also accepted.
- ü§ó Pre-trained Model Support: We provide ready-to-use scripts for off-the-shelf methods: Depth Anything V2 (`disparity`) and Marigold (`affine-invariant`). These will serve as a competitive baseline for the challenge and a starting point for participants.
- üìä Updated Evaluation Pipeline: The CodaLab grader code has been updated to accommodate the newly supported prediction types.

### üöÄ How to participate? {#participate}

1. Check out the new starter pack [GitHub](https://github.com/toshas/mdec_benchmark). The [mdec_2025](https://github.com/toshas/mdec_benchmark/tree/main/mdec_2025) folder contains scripts generating valid submissions for [Marigold](https://github.com/toshas/mdec_benchmark/blob/main/mdec_2025/marigold_v1-0/generate.py) (`affine-invariant`) and [Depth Anything v2](https://github.com/toshas/mdec_benchmark/blob/main/mdec_2025/depth_anything_v2/generate.py) (`disparity`).
2. Identify the prediction type of your method and generate a valid submission: `val` split for the "Development" phase and `test` split for the "Final" phase.
3. Register at the [CodaLab Challenge](https://codalab.lisn.upsaclay.fr/competitions/21305) site, check the submission constraints and extra conditions, and submit to the leaderboard.

The phases are open according to the following schedule:
- "Development": Feb 01 - Mar 01
- "Final": Mar 01 - Mar 21

### üìä Evaluation {#evaluation}

Submissions will be evaluated on a variety of metrics:
- [Pointcloud reconstruction](https://arxiv.org/abs/2203.08122): F-Score
- [Image-based depth](https://arxiv.org/abs/1708.06500): MAE, RMSE, AbsRel
- [Depth discontinuities](https://arxiv.org/abs/1805.01328v1): F-Score, Accuracy, Completeness

The **leading metric** is **F-Score** (based on the point cloud), denoted as **F&nbsp;(‚Üë)** in the leaderboard.
Challenge winners will be determined based on the performance ranked by the leading metric on the withheld validation ("Development" phase) and the test ("Final" phase) sets of the SYNS-Patches dataset.

To measure the performance locally with other datasets or troubleshoot scoring issues within the challenge, refer to the [evaluation code](https://github.com/toshas/mdec_benchmark/blob/main/src/core/evaluator.py).

### üìà Baselines {#baselines}

This year, we switched to LSE-based alignment between predictions and ground truth maps to accept various types of predictions. 
In addition to previously accepted `disparity` prediction methods, we welcome `affine-invariant`, `scale-invariant`, and `metric` types.

Accordingly, we updated the benchmark with more recent baselines, such as Marigold (`affine-invariant`), Depth Anything v2 (`disparity`), and the winners of the 3rd edition of the MDEC challenge, whose performances are reported below.

|                |      F&nbsp;(‚Üë)                                                                | F&nbsp;(‚Üë)<br/>(Edges)                                                    | MAE&nbsp;(‚Üì)                                                             | RMSE&nbsp;(‚Üì)                                                            | AbsRel&nbsp;(‚Üì)                                                              | Acc&nbsp;(‚Üë)<br/>(Edges)                                                 | Comp&nbsp;(‚Üì)<br/>(Edges)                                                | Œ¥<1.25&nbsp;(‚Üë)                                                | Œ¥<1.25^2&nbsp;(‚Üë)                                                | Œ¥<1.25^3&nbsp;(‚Üë)                                                | 
|----------------|------|------------------------------------------------------------------|------------------------------------------------------------------|-----------------------------------------------------------------|-----------------------------------------------------------------|------------------------------------------------------------------|-----------------------------------------------------------------|-----------------------------------------------------------------|
| **PICO-MR**    | 21.07          | 8.77 | 3.22 | 5.60 | 20.33                                                            | 3.69                                                            | 15.41                                                            | 0.7559                                                            | 0.9125                                                            | 0.9590                                                   
| **EVP++**    | 19.66          | 9.02 | 3.20 | 5.49 | 19.03                                                            | 2.66                                                            | 9.28                                                            | 0.7553                                                            | 0.9182                                                            | 0.9661                                                  
| **Marigold**    | 18.64          | 9.26 | 3.87 | 6.49 | 24.37                                                            | 2.90                                                            | 20.09                                                            | 0.6903                                                            | 0.8860                                                            | 0.9453                                                  
| **Depth Anything v2**    | 14.34          | 7.94 | 4.16 | 7.94 | 25.48                                                            | 2.64                                                            | 30.05                                                            | 0.6907                                                            | 0.8849                                                            | 0.9469                                                  
| **Garg's Baseline**    | 11.38          | 6.03 | 4.62 | 7.58 | 31.15                                                            | 4.01                                                            | 41.24                                                            | 0.5842                                                            | 0.8354                                                            | 0.9251

### üìö Workshop proceedings {#proceedings}

As part of the CVPR Workshop Proceedings, we will publish a paper summarizing the results of the challenge. The following conditions must be met to have the method included in the paper:

- The method surpasses the performance of the baselines in the leading metric (F-Score);
- The method should not be trivial;
- Each prediction is made using a single corresponding input image;

Once the challenge has finished, we will contact the participants meeting the criteria above to request information about their affiliation, a short description of their method, and the method's source code. Participants not providing this information will not be added to the publication; their submission will stay anonymous in the leaderboard.

Selected top performers will also be invited to present their methods at the workshop. The presentation can be held either in person or virtually. This is mandatory; refusal to do so will result in an invalidated submission and removal from the paper.

## Feedback {#feedback}

Please feel free to reach out with any questions, concerns, or feedback using the address below ‚Äî this is the quickest way to contact us. If your topic relates to the challenge, in addition to emailing us, consider opening a discussion on the [CodaLab forum](https://codalab.lisn.upsaclay.fr/forums/21249/).

<img src="assets/imgs/feedback.png" height=32px alt="Feedback address"/> -->

---

## ü§µ **Organizers** {#organizers}
<div class="container">
<figure>
    <a href="https://yiyiliao.github.io/">
    <img class="img-author" src="https://yiyiliao.github.io/assets/img/prof_pic.jpg" alt="Yiyi Liao"/></a>
    <b><br><a href="https://yiyiliao.github.io/">Yiyi Liao</a>
    <br>Zhejiang University</b>
</figure>
    
<figure>
    <a href="https://hyzhou404.github.io/">
    <img class="img-author" src="assets/imgs/authors/hongyu_zhou.jpg" alt="Hongyu Zhou"/></a>
    <b><br><a href="https://hyzhou404.github.io/">Hongyu Zhou</a>
    <br>Zhejiang University</b>
</figure>

<figure>
    <a href="https://yichonglu.github.io/">
    <img class="img-author" src="assets/imgs/authors/Yichong_Lu.jpg" alt="Yichong Lu"/></a>
    <b><br><a href="https://yichonglu.github.io/">Yichong Lu</a>
    <br>Zhejiang University</b>
</figure>

<figure>
    <a href="https://www.aminer.cn/profile/liu-bingbing/562d234145cedb3398d63523">
    <img class="img-author" src="assets/imgs/authors/Bingbing_Liu.png" alt="Bingbing Liu"/></a>
    <b><br><a href="https://www.aminer.cn/profile/liu-bingbing/562d234145cedb3398d63523">Bingbing Liu</a>
    <br>Huawei</b>
</figure>

<figure>
    <a href="https://ieeexplore.ieee.org/author/37859161500">
    <img class="img-author" src="assets/imgs/authors/hongbo_zhang.jpg" alt="Hongbo Zhang"/></a>
    <b><br><a href="https://ieeexplore.ieee.org/author/37859161500">Hongbo Zhang</a>
    <br>Huawei</b>
</figure>

<figure>
    <a href="https://openreview.net/profile?id=~Jiansheng_Wei1">
    <img class="img-author" src="assets/imgs/authors/jiansheng_wei.png" alt="Jiansheng Wei"/></a>
    <b><br><a href="https://openreview.net/profile?id=~Jiansheng_Wei1">Jiansheng Wei</a>
    <br>Huawei</b>
</figure>



<figure>
    <a href="https://openreview.net/profile?id=%7EZiqian_Ni1">
    <img class="img-author" src="assets/imgs/authors/ziqian_ni.jpg" alt="Ziqian Ni"/></a>
    <b><br><a href="https://openreview.net/profile?id=%7EZiqian_Ni1">Ziqian Ni</a>
    <br>Cainiao</b>
</figure>

<figure>
    <a href="https://yimingli-page.github.io/">
    <img class="img-author" src="assets/imgs/authors/yiming_li.jpeg" alt="Yiming Li"/></a>
    <b><br><a href="https://yimingli-page.github.io/">Yiming Li</a>
    <br>NVIDIA & NYU</b>
</figure>

<figure>
    <a href="https://www.cvlibs.net/">
    <img class="img-author" src="assets/imgs/authors/Andreas.jpg" alt="Andreas Geiger"/></a>
    <b><br><a href="https://www.cvlibs.net/">Andreas Geiger</a>
    <br>University of T√ºbingen</b>
</figure>
</div>

